{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import importlib\n",
    "def reload_pack():\n",
    "    from tools import inference\n",
    "    importlib.reload(inference)\n",
    "    from tools import utils\n",
    "    importlib.reload(utils)\n",
    "    from tools import transformers_patch\n",
    "    importlib.reload(transformers_patch)\n",
    "    from tools import graphic\n",
    "    importlib.reload(graphic)\n",
    "reload_pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "from tools.utils import load_model\n",
    "\n",
    "## TO BE REMOVED\n",
    "from pathlib import Path\n",
    "import os\n",
    "working_directory = Path(os.getcwd())\n",
    "folder_path = working_directory.parent / 'Models'\n",
    "path = folder_path / 'llama-3-hf/8B'\n",
    "print(f'Loading model from: {path}')\n",
    "## TO BE REMOVED END\n",
    "\n",
    "tokenizer, model = load_model(path, model_type='llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.utils import gen_prompt, show_outputs\n",
    "from tools.inference import forward\n",
    "\n",
    "prompt_comp = gen_prompt(task='country2capital')\n",
    "prompt = prompt_comp['icl']+prompt_comp['ans']\n",
    "print(f'Correct:\\n{prompt}')\n",
    "\n",
    "gen_len = 2\n",
    "outputs = forward(model, tokenizer, [prompt_comp['icl']], gen_len=gen_len)\n",
    "print(f'--------\\nGenerated:\\n{show_outputs(tokenizer, outputs.sequences)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "def get_attention_layers(model: PreTrainedModel):\n",
    "    layers = model.model.layers\n",
    "    return [layer.self_attn for layer in layers]\n",
    "\n",
    "class AttentionReweighter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        attention_reweight\n",
    "    ):\n",
    "        self._model = model\n",
    "        self.attention_reweight = attention_reweight\n",
    "        self._hooks = []\n",
    "        self.causal_mask_org = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._register_forward_pre_hooks()\n",
    "        return\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        for hook in self._hooks:\n",
    "            hook.remove()\n",
    "        self._hooks = []\n",
    "\n",
    "    def _register_forward_pre_hooks(self):\n",
    "        def attn_mask_hook(layer_idx):\n",
    "            def mask_attn(mod, inp):\n",
    "                if layer_idx == 0:\n",
    "                    self.causal_mask_org = inp[1].clone()\n",
    "                    # 实际上已经不需要了\n",
    "                        \n",
    "                causal_mask = self.causal_mask_org\n",
    "                attention_reweight = self.attention_reweight[:, layer_idx].unsqueeze(dim=1)\n",
    "\n",
    "                causal_mask_size = causal_mask.shape[-2] \n",
    "                attn_mask_size = attention_reweight.shape[-2]\n",
    "                target_idxs = torch.tensor(range(attn_mask_size))\n",
    "                attn_source_idx = torch.tensor(range(attn_mask_size-causal_mask_size, attn_mask_size))\n",
    "                causal_source_idx = torch.tensor(range(causal_mask_size))\n",
    "\n",
    "                causal_mask[:, :, causal_source_idx[:, None], target_idxs] = attention_reweight[:,:,attn_source_idx[:, None], target_idxs]\n",
    "\n",
    "                inp = tuple((inp[0], causal_mask))\n",
    "                return inp\n",
    "            return mask_attn\n",
    "\n",
    "        for i, layer in enumerate(get_attention_layers(self._model)):\n",
    "            hook = layer.register_forward_pre_hook(attn_mask_hook(i))\n",
    "            self._hooks.append(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from typing import List, Callable\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors='pt', padding=True).to(model.device)\n",
    "prompt_ids = inputs['input_ids']\n",
    "input_ids = prompt_ids[:, :-1]\n",
    "\n",
    "batch_size = prompt_ids.shape[0]\n",
    "num_tokens = prompt_ids.shape[1]\n",
    "\n",
    "# Get original probabilities for later loss calculation.\n",
    "model.eval()\n",
    "outputs = model(input_ids)\n",
    "logits0 = outputs.logits[:, -gen_len:].detach().clone()\n",
    "\n",
    "# Initialize the attention mask.\n",
    "attn_mask = torch.ones(batch_size, 32, num_tokens, num_tokens, device=model.device)\n",
    "\n",
    "# Setting attn_mask to be a trainable parameter.\n",
    "attn_mask = nn.Parameter(attn_mask)\n",
    "\n",
    "# For GD, we do not update connections towards bos token and connention to itself.\n",
    "update_mask = torch.tril(torch.ones_like(attn_mask), diagonal=-1).bool() # Remove self and connections under causal mask.\n",
    "padding_mask = (1-inputs['attention_mask'])[:, None, None, :].to(bool).repeat(1, 32, num_tokens, 1)\n",
    "update_mask[padding_mask] = 0 # Remove padding tokens.\n",
    "# Remove bos tokens. (CAN BE IMPROVED)\n",
    "update_mask[:,:,:,0] = 0\n",
    "for batch_idx in range(batch_size):\n",
    "    for i in range(num_tokens):\n",
    "        if padding_mask[batch_idx, 0, -1, i]:\n",
    "            update_mask[batch_idx, :, :, i+1] = 0\n",
    "\n",
    "# Set certain connection weight to -\\infty, for causal connections and padding tokens.\n",
    "min_type = torch.finfo(attn_mask.dtype).min\n",
    "\n",
    "# We use L2 loss to measure the gap between modified probabilities and orginal probabilities.\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "block_rate = 1.\n",
    "gap = 0.\n",
    "gap_ckpt = 1000.\n",
    "\n",
    "warning_flag = False\n",
    "epoch = 0\n",
    "epoch_ckpt = 0\n",
    "attn_mask_ckpt = attn_mask.data.detach().clone()\n",
    "\n",
    "model.train()\n",
    "record = []\n",
    "print(f'logits: {logits0.max(dim=-1).values}')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.SGD([attn_mask],lr=1e-3)\n",
    "regular = 1e3\n",
    "lower_bound = 1e-12\n",
    "for epoch in range(2000):\n",
    "    # Apply attn_mask in the inference process.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    attn_mask_log_causal = torch.log(torch.clamp(attn_mask, min=lower_bound)).masked_fill(~update_mask, min_type)\n",
    "    modified_forward = AttentionReweighter(model, attn_mask_log_causal)\n",
    "    with modified_forward:\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, -gen_len:]\n",
    "\n",
    "    # Gradient Descent\n",
    "    sparse = attn_mask[update_mask].sum()\n",
    "    loss_raw = loss_fn(logits, logits0)*gen_len\n",
    "    loss = regular*loss_raw + sparse\n",
    "    loss.backward()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        optimizer.step()\n",
    "        attn_mask.clamp_(min=0, max=1)\n",
    "\n",
    "        block_rate = attn_mask[update_mask].mean().item()\n",
    "        gap = loss_raw.sqrt().item()\n",
    "\n",
    "        view_gap = 100\n",
    "        if (epoch+1)%view_gap==0:\n",
    "            print(f\"Epoch: {epoch}, Block rate: {block_rate:.3f}, logits: {logits[0,-1].max().item():.3f}, Target logits: {logits0[0,-1].max().item():.3f}, Gap: {gap:.3f}\")\n",
    "            # record.append({'epoch':epoch,'attn_mask':attn_mask.detach().clone(), 'sparsity': block_rate, 'gap': gap})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.graphic import demonstrate_by_token\n",
    "\n",
    "labels = []\n",
    "for t in prompt_ids[0]:\n",
    "    labels.append(tokenizer.decode(t))\n",
    "\n",
    "connection = attn_mask[0,:,-gen_len]\n",
    "demonstrate_by_token(connection, labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
